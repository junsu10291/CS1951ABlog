{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "##### Variables #####\n",
    "K_CLUSTERS_RESTAURANT = 20\n",
    "K_CLUSTERS_USER = 20\n",
    "REDUCED_D = 300\n",
    "TERMS_PER_CLUSTER_RESTAURANT = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'stop_words'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-bdc333dedb55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msqlite3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_stop_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'stop_words'"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "from __future__ import print_function\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import logging\n",
    "from time import time\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from stop_words import get_stop_words\n",
    "from collections import Counter\n",
    "from pandas import merge\n",
    "\n",
    "%matplotlib notebook\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"yelp2.db\")\n",
    "df = pd.read_sql_query(\"select text, user_id, business_id, stars from review;\", conn)\n",
    "user_id_valcounts = df['user_id'].value_counts()\n",
    "unique_users = user_id_valcounts[user_id_valcounts >= 50].index.tolist()\n",
    "df = df[df['user_id'].isin(unique_users)] #unique_users:  558, unique restaurants:  1620\n",
    "unique_business = list(set(df['business_id']))\n",
    "\n",
    "dict_business= {}\n",
    "dict_user = {}\n",
    "for index, row in df.iterrows():\n",
    "    business = str(row['business_id'])\n",
    "    user = str(row ['user_id'])\n",
    "    review = str(row['text']).replace('\\n','')\n",
    "    if business in dict_business:\n",
    "        dict_business[business] += review\n",
    "    else:\n",
    "        dict_business[business] = review\t\n",
    "    if user in dict_user:\n",
    "        dict_user[user] += review\n",
    "    else:\n",
    "        dict_user[user] = review\n",
    "rawdata_user = [ [k1,v1] for k1, v1 in dict_user.items() ]\n",
    "rawdata_busi = [ [k2,v2] for k2, v2 in dict_business.items() ]\n",
    "\n",
    "en_stop = get_stop_words('en')\n",
    "data_b=[]\n",
    "data_u=[]\n",
    "for entry in rawdata_user:\n",
    "    nonstop = []\n",
    "    for word in entry[1].split():\n",
    "        if word not in en_stop:\n",
    "            nonstop.append(word)\n",
    "    stopresult = ' '.join(nonstop)\n",
    "    entry[1] = stopresult\n",
    "    data_u.append(stopresult)\n",
    "    \n",
    "for entry in rawdata_busi:\n",
    "    nonstop = []\n",
    "    for word in entry[1].split():\n",
    "        if word not in en_stop:\n",
    "            nonstop.append(word)\n",
    "    stopresult = ' '.join(nonstop)\n",
    "    entry[1] = stopresult\n",
    "    data_b.append(stopresult)\n",
    "\n",
    "   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Users Clustered\n",
    "\n",
    "#########################################\n",
    "\n",
    "print(\"Extracting features from the training dataset using a sparse vectorizer\")\n",
    "MAX_FEATURES = 100000\n",
    "vectorizer = TfidfVectorizer(max_df=0.7, max_features=MAX_FEATURES,\n",
    "                             min_df=100, stop_words='english',\n",
    "                             use_idf=True)\n",
    "X = vectorizer.fit_transform(data_u)\n",
    "print(\"n_samples: %d, n_features: %d\" % X.shape)\n",
    "\n",
    "#########################################\n",
    "\n",
    "print(\"Performing dimensionality reduction using LSA\")\n",
    "svd = TruncatedSVD(REDUCED_D)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "lsa_X = lsa.fit_transform(X)\n",
    "explained_variance = svd.explained_variance_ratio_.sum()\n",
    "print(\"(Reduced) n_samples: %d, n_features: %d\" % lsa_X.shape)\n",
    "print(\"Explained variance of the SVD step: {}%\".format(\n",
    "    int(explained_variance * 100)))\n",
    "\n",
    "#########################################\n",
    "\n",
    "print(\"Train a kmeans classifier\")\n",
    "km = MiniBatchKMeans(n_clusters=K_CLUSTERS_USER, init='k-means++', n_init=5,\n",
    "                     init_size=100000, batch_size=1000, verbose=True)\n",
    "km.fit(lsa_X)\n",
    "print(\"Top %d terms per cluster:\" %TERMS_PER_CLUSTER_RESTAURANT)\n",
    "original_space_centroids = svd.inverse_transform(km.cluster_centers_)\n",
    "order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(K_CLUSTERS_RESTAURANT):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in order_centroids[i, :TERMS_PER_CLUSTER_RESTAURANT]:\n",
    "        print(' %s' % terms[ind], end='')\n",
    "    print()\n",
    "\n",
    "#########################################\n",
    "\n",
    "clusters = km.labels_.tolist()\n",
    "print(\"Number of users in each cluster: \", Counter(clusters))\n",
    "df.insert(len(df.columns),'user_cluster',0)\n",
    "for index, row  in df.iterrows():\n",
    "    uid = str(row['user_id'])\n",
    "    ind = -1\n",
    "    for i in range(0, len(rawdata_user)):\n",
    "        if(uid == str(rawdata_user[i][0])):\n",
    "            ind = i\n",
    "    df.set_value(index, 'user_cluster', clusters[ind])\n",
    "    \n",
    "#########################################\n",
    "\n",
    "user_id_cluster = df['user_cluster'].groupby(df['user_id']).agg(lambda x:x.value_counts().index[0])\n",
    "label = user_id_cluster.tolist()\n",
    "unique_users.sort()\n",
    "user_cluster = {'user_id': unique_users, 'user_cluster': label}\n",
    "df_user_cluster = pd.DataFrame(user_cluster, columns = {'user_id', 'user_cluster'})\n",
    "print('User Cluster Count: ', df_user_cluster['user_cluster'].value_counts())\n",
    "\n",
    "#c_temp = df.columns.difference(df_user_cluster.columns)\n",
    "combined = merge(df, df_user_cluster)\n",
    "\n",
    "#########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Restuarants Clustered \n",
    "\n",
    "#########################################\n",
    "print(\"Extracting features from the training dataset using a sparse vectorizer\")\n",
    "MAX_FEATURES = 100000\n",
    "vectorizer2 = TfidfVectorizer(max_df=0.7, max_features=MAX_FEATURES,\n",
    "                             min_df=100, stop_words='english',\n",
    "                             use_idf=True)\n",
    "X2 = vectorizer2.fit_transform(data_b)\n",
    "print(\"n_samples: %d, n_features: %d\" % X2.shape)\n",
    "\n",
    "#########################################\n",
    "\n",
    "print(\"Performing dimensionality reduction using LSA\")\n",
    "svd = TruncatedSVD(REDUCED_D)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "lsa_X2 = lsa.fit_transform(X2)\n",
    "explained_variance = svd.explained_variance_ratio_.sum()\n",
    "print(\"(Reduced) n_samples: %d, n_features: %d\" % lsa_X2.shape)\n",
    "print(\"Explained variance of the SVD step: {}%\".format(\n",
    "    int(explained_variance * 100)))\n",
    "\n",
    "#########################################\n",
    "\n",
    "print(\"Train a kmeans classifier\")\n",
    "km2 = MiniBatchKMeans(n_clusters=K_CLUSTERS_RESTAURANT, init='k-means++', n_init=5,\n",
    "                     init_size=100000, batch_size=1000, verbose=True)\n",
    "km2.fit(lsa_X2)\n",
    "print(\"Top %d terms per cluster:\" %TERMS_PER_CLUSTER_RESTAURANT)\n",
    "original_space_centroids = svd.inverse_transform(km2.cluster_centers_)\n",
    "order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "terms2 = vectorizer2.get_feature_names()\n",
    "terms_cluster = []\n",
    "for i in range(K_CLUSTERS_RESTAURANT):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    temp = []\n",
    "    for ind in order_centroids[i, :TERMS_PER_CLUSTER_RESTAURANT]:\n",
    "        print(' %s' % terms2[ind], end='')\n",
    "        temp.append(terms2[ind])\n",
    "    terms_cluster.append(temp)\n",
    "    print()\n",
    "    \n",
    "#########################################\n",
    "\n",
    "clusters2 = km2.labels_.tolist()\n",
    "print(\"Number of users in each cluster: \", Counter(clusters2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for index, row  in df.iterrows():\n",
    "    bid = str(row['business_id'])\n",
    "    ind = -1\n",
    "    for i in range(0, len(rawdata_busi)):\n",
    "        if(bid == str(rawdata_busi[i][0])):\n",
    "            ind = i\n",
    "    df.set_value(index, 'restaurant_cluster', int(clusters2[ind]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for index, row  in df.iterrows():\n",
    "    bid = str(row['business_id'])\n",
    "    ind = -1\n",
    "    for i in range(0, len(rawdata_busi)):\n",
    "        if(bid == str(rawdata_busi[i][0])):\n",
    "            ind = i\n",
    "    df.set_value(index, 'restaurant_cluster', int(clusters2[ind]))\n",
    "    \n",
    "#########################################\n",
    "\n",
    "restaurant_id_cluster = df['restaurant_cluster'].groupby(df['business_id']).agg(lambda x:x.value_counts().index[0])\n",
    "label2 = restaurant_id_cluster.tolist()\n",
    "unique_business.sort()\n",
    "restaurant_cluster = {'business_id': unique_business, 'restaurant_cluster': label2}\n",
    "df_restaurant_cluster = pd.DataFrame(restaurant_cluster, columns = {'business_id', 'restaurant_cluster'})\n",
    "print('Restaurant Cluster Count: ', df_restaurant_cluster['restaurant_cluster'].value_counts())\n",
    "\n",
    "#c_temp2 = df.columns.difference(df_restaurant_cluster.columns)\n",
    "combined = merge(df, df_restaurant_cluster)\n",
    "\n",
    "#########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "temp = []\n",
    "for i in range(K_CLUSTERS_RESTAURANT):\n",
    "    temp.append(i)\n",
    "terms_c = {'restuarant_cluster': temp, 'terms': terms_cluster}\n",
    "df_term_cluster = pd.DataFrame(terms_c, columns = {'restuarant_cluster', 'terms'})\n",
    "df_term_cluster.to_csv('restaurant_cluster_terms.csv')\n",
    "combined.to_csv('preclustered.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
